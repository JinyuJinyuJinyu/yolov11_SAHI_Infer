{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from util import non_max_suppression\n",
    "import os\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a227a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def preprocess_image(image_path, input_size=640):\n",
    "    \"\"\"Preprocess image to match training pipeline in Yolo_Dataset.\"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "    \n",
    "    # Get original shape\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Scale ratio (new / old)\n",
    "    r = min(input_size / h, input_size / w)\n",
    "    \n",
    "    # Resize\n",
    "    pad = (int(w * r), int(h * r))\n",
    "    if (h, w) != pad[::-1]:\n",
    "        image = cv2.resize(image, dsize=pad, interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Compute padding\n",
    "    top, bottom = int((input_size - pad[1]) / 2), int((input_size - pad[1]) / 2 + (input_size - pad[1]) % 2)\n",
    "    left, right = int((input_size - pad[0]) / 2), int((input_size - pad[0]) / 2 + (input_size - pad[0]) % 2)\n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "    \n",
    "    # Convert HWC to CHW, BGR to RGB, normalize\n",
    "    image = image.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    image = np.ascontiguousarray(image) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # Convert to float32 for ONNX\n",
    "    image = image.astype(np.float32)\n",
    "    \n",
    "    return image, (r, r), (left, top), (h, w)\n",
    "\n",
    "def draw_detections(image, detections, class_names=None):\n",
    "    \"\"\"Draw bounding boxes and labels on the image.\"\"\"\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "        label = f\"{int(cls)} {conf:.2f}\" if class_names is None else f\"{class_names[int(cls)]} {conf:.2f}\"\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return image\n",
    "\n",
    "def onnx_inference(session, image_paths, input_size=640, conf_thres=0.25, iou_thres=0.65, output_dir=\"output_onnx\"):\n",
    "    \"\"\"Run inference on a list of images using ONNX model and save results.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for image_path in tqdm(image_paths, desc=\"ONNX Inference\"):\n",
    "        # Preprocess image\n",
    "        image_np, ratio, pad, orig_shape = preprocess_image(image_path, input_size)\n",
    "        image_np = image_np[np.newaxis, ...]  # Add batch dimension (1, 3, 640, 640)\n",
    "        \n",
    "        # Run ONNX inference\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        output_name = session.get_outputs()[0].name\n",
    "        outputs = session.run([output_name], {input_name: image_np})[0]\n",
    "        \n",
    "        # Convert ONNX output to PyTorch tensor for NMS\n",
    "        outputs = torch.from_numpy(outputs).to(DEVICE)\n",
    "        \n",
    "        # Apply non-maximum suppression\n",
    "        detections = non_max_suppression(outputs, confidence_threshold=conf_thres, iou_threshold=iou_thres)\n",
    "        \n",
    "        # Process detections\n",
    "        detections = detections[0]  # First batch\n",
    "        if detections.shape[0] > 0:\n",
    "            # Rescale boxes to original image size\n",
    "            detections[:, :4] = detections[:, :4].clone()\n",
    "            detections[:, [0, 2]] = (detections[:, [0, 2]] - pad[0]) / ratio[0]  # x1, x2\n",
    "            detections[:, [1, 3]] = (detections[:, [1, 3]] - pad[1]) / ratio[1]  # y1, y2\n",
    "        \n",
    "        # Load original image for visualization\n",
    "        orig_image = cv2.imread(image_path)\n",
    "        if detections.shape[0] > 0:\n",
    "            orig_image = draw_detections(orig_image, detections)\n",
    "        else:\n",
    "            print(f\"No detections for {image_path}\")\n",
    "        \n",
    "        # Save output\n",
    "        output_path = os.path.join(output_dir, os.path.basename(image_path))\n",
    "        cv2.imwrite(output_path, orig_image)\n",
    "    \n",
    "    print(f\"Results saved to {output_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1846ed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ONNX model from /mnt/DATA/gits/yolov11_scratch/weights/my_model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX Inference:   0%|          | 1/721 [00:01<19:16,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240114104931_0053_T_-_JPG.rf.6b097ea0b5143cc9b32f7166ce5d8247.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX Inference:   1%|          | 9/721 [00:02<01:35,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240114112233_0038_T_-_JPG.rf.edc7f15cae23acb0f83339c86ad941b6.jpg\n",
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240121114851_0016_T_-_JPG.rf.e5744025d74d1e294551ef054eb3ec36.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX Inference:   3%|▎         | 24/721 [00:02<00:33, 21.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240114105337_0053_T_-_JPG.rf.ff5a7e0f6af4a2bfa16a1e5c224815a2.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX Inference:   4%|▍         | 28/721 [00:02<00:29, 23.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240113121331_0039_T_-_JPG.rf.2e2ef160a7b78e1a776c1599b31ace44.jpg\n",
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240114105432_0060_T_-_JPG.rf.45ab147a48e22ff6f8703ac111b2f4e7.jpg\n",
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240114113322_0074_T_-_JPG.rf.9eea5a966092d77fb8ed091386a54adb.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX Inference:   8%|▊         | 57/721 [00:03<00:23, 28.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240114104815_0009_T_-_JPG.rf.3ad598ff3f0afada6f9eebe72a9d8b0b.jpg\n",
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240114103752_0082_T_-_JPG.rf.dc1ea173a2c36f81a1e009eefcbcf1ee.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX Inference:  10%|▉         | 69/721 [00:04<00:22, 28.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240603133248_0074_T_-_jpeg.rf.f255728c72c239926cdf25ed7684b116.jpg\n",
      "No detections for /mnt/DATA/DATASETS/data/dlpj/slp/val/images/DJI_20240121120733_0069_T_-_JPG.rf.7aeffb59dd085478bea54902fc212c9b.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ONNX Inference:  10%|▉         | 71/721 [00:04<00:39, 16.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33moutput_onnx/inference_results\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43monnx_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_thres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_thres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36monnx_inference\u001b[39m\u001b[34m(session, image_paths, input_size, conf_thres, iou_thres, output_dir)\u001b[39m\n\u001b[32m     56\u001b[39m input_name = session.get_inputs()[\u001b[32m0\u001b[39m].name\n\u001b[32m     57\u001b[39m output_name = session.get_outputs()[\u001b[32m0\u001b[39m].name\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m outputs = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Convert ONNX output to PyTorch tensor for NMS\u001b[39;00m\n\u001b[32m     61\u001b[39m outputs = torch.from_numpy(outputs).to(DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/tc11/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    218\u001b[39m     output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m C.EPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"/mnt/DATA/gits/yolov11_scratch/weights/my_model.onnx\"\n",
    "\n",
    "# Initialize ONNX runtime session\n",
    "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n",
    "try:\n",
    "    session = ort.InferenceSession(onnx_model_path, providers=providers)\n",
    "    print(f\"Loaded ONNX model from {onnx_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load ONNX model: {e}\")\n",
    "\n",
    "\n",
    "# Input images (e.g., validation set or test images)\n",
    "image_paths = glob('/mnt/DATA/DATASETS/data/dlpj/slp/val/images/*.jpg')  # Adjust path as needed\n",
    "\n",
    "# Inference parameters\n",
    "input_size = 640\n",
    "conf_thres = 0.1  # Lowered to allow more detections\n",
    "iou_thres = 0.5   # Lowered to reduce NMS filtering\n",
    "output_dir = \"output_onnx/inference_results\"\n",
    "\n",
    "# Run inference\n",
    "onnx_inference(session, image_paths, input_size, conf_thres, iou_thres, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tc11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
